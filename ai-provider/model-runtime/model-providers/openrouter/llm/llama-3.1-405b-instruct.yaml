model: meta-llama/llama-3.1-405b-instruct
label:
  en_US: llama-3.1-405b-instruct
model_type: llm
model_properties:
  mode: chat
  context_size: 131072
parameter_rules:
  - name: temperature
    use_template: temperature
  - name: top_p
    use_template: top_p
  - name: max_tokens
    use_template: max_tokens
    required: true
    default: 512
    min: 1
    max: 131072
pricing:
  input: "2.7"
  output: "2.7"
  unit: "0.000001"
  currency: USD
