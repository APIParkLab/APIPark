model: 01-ai/Yi-1.5-9B-Chat-16K
label:
  en_US: 01-ai/Yi-1.5-9B-Chat-16K
model_type: llm
features:
  - agent-thought
model_properties:
  mode: chat
  context_size: 16384
parameter_rules:
  - name: temperature
    use_template: temperature
  - name: max_tokens
    use_template: max_tokens
    type: int
    default: 512
    min: 1
    max: 4096
    help:
      zh_Hans: 指定生成结果长度的上限。如果生成结果截断，可以调大该参数。
      en_US: Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.
  - name: top_p
    use_template: top_p
  - name: frequency_penalty
    use_template: frequency_penalty
pricing:
  input: '0'
  output: '0'
  unit: '0.000001'
  currency: RMB
